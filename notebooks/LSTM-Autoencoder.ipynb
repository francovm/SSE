{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard,CSVLogger\n",
    "from keras.models import load_model\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(7)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123 #used to help randomly select the data points\n",
    "DATA_SPLIT_PCT = 0.2\n",
    "\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "LABELS = [\"Normal\",\"Break\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  dataset\n",
    "df = pd.read_csv('...input_data_processed.csv', sep='\\t', encoding='utf-8', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = df.loc[:, df.columns != 'Events'].values  # converts the df to a numpy array\n",
    "input_y = df['Events'].values\n",
    "\n",
    "n_features = 3  # number of features\n",
    "\n",
    "print(input_X.shape,input_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3d array (tensor) from the time series\n",
    "def temporalize(X, y, lookback):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(input_X)-lookback-1):\n",
    "        t = []\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            t.append(input_X[[(i+j+1)], :])\n",
    "        X.append(t)\n",
    "        y.append(input_y[i+lookback+1])\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporalize the data\n",
    "lookback = 40\n",
    "X, y = temporalize(X = input_X, y = input_y, lookback = lookback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into Test, valid and train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=DATA_SPLIT_PCT, random_state=SEED)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=DATA_SPLIT_PCT, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_y0 = X_train[y_train==0.]\n",
    "X_train_y1 = X_train[y_train==1.]\n",
    "\n",
    "X_valid_y0 = X_valid[y_valid==0.]\n",
    "X_valid_y1 = X_valid[y_valid==1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the arrays\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], lookback, n_features)\n",
    "X_train_y0 = X_train_y0.reshape(X_train_y0.shape[0], lookback, n_features)\n",
    "X_train_y1 = X_train_y1.reshape(X_train_y1.shape[0], lookback, n_features)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], lookback, n_features)\n",
    "X_valid_y0 = X_valid_y0.reshape(X_valid_y0.shape[0], lookback, n_features)\n",
    "X_valid_y1 = X_valid_y1.reshape(X_valid_y1.shape[0], lookback, n_features)\n",
    "X_test = X_test.reshape(X_test.shape[0], lookback, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_y0.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = X_train.shape[1], 3, 1\n",
    "print(n_timesteps,n_features,n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    '''\n",
    "    Flatten a 3D array.\n",
    "    \n",
    "    Input\n",
    "    X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
    "    \n",
    "    Output\n",
    "    flattened_X  A 2D array, sample x features.\n",
    "    '''\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "\n",
    "def scale(X, scaler):\n",
    "    '''\n",
    "    Scale 3D array.\n",
    "\n",
    "    Inputs\n",
    "    X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
    "    scaler       A scaler object, e.g., sklearn.preprocessing.StandardScaler, sklearn.preprocessing.normalize\n",
    "    \n",
    "    Output\n",
    "    X            Scaled 3D array.\n",
    "    '''\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i, :, :] = scaler.transform(X[i, :, :])\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stardarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a scaler using the training data.\n",
    "scaler = StandardScaler().fit(flatten(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the input\n",
    "X_train_y0_scaled = scale(X_train_y0, scaler)\n",
    "X_valid_y0_scaled = scale(X_valid_y0, scaler)\n",
    "X_test_scaled = scale(X_test, scaler)\n",
    "X_train_scaled = scale(X_train, scaler)\n",
    "X_valid_scaled = scale(X_valid, scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = flatten(X_train_scaled)\n",
    "print('colwise mean', np.mean(a,axis=0).round(6))\n",
    "print('colwise variance', np.var(a, axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.0001\n",
    "verbose, epochs, batch_size = 1, 400, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_autoencoder = Sequential()\n",
    "# Encoder\n",
    "lstm_autoencoder.add(LSTM(128, activation='relu', input_shape=(n_timesteps, n_features), return_sequences=True))\n",
    "lstm_autoencoder.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "lstm_autoencoder.add(RepeatVector(n_timesteps))\n",
    "# Decoder\n",
    "lstm_autoencoder.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "lstm_autoencoder.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "lstm_autoencoder.add(TimeDistributed(Dense(n_features)))\n",
    "\n",
    "lstm_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = optimizers.Adam(lr)\n",
    "lstm_autoencoder.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#set early stopping monitor so the model stops training when it won't improve anymore\n",
    "early_stopping_monitor = EarlyStopping(patience=5)\n",
    "\n",
    "\n",
    "lstm_autoencoder_history = lstm_autoencoder.fit(X_train_y0_scaled, X_train_y0_scaled, \n",
    "                                                epochs=epochs, \n",
    "                                                batch_size=batch_size, \n",
    "                                                callbacks=[early_stopping_monitor],\n",
    "                                                validation_data=(X_valid_y0_scaled, X_valid_y0_scaled),\n",
    "                                                verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(lstm_autoencoder_history.history.keys())\n",
    "\n",
    "# # summarize history for accuracy\n",
    "plt.plot(lstm_autoencoder_history.history['accuracy'])\n",
    "plt.plot(lstm_autoencoder_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "#  Set figure width to 18 and height to 4\n",
    "fig_size[0] = 18\n",
    "fig_size[1] = 4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(lstm_autoencoder_history.history['loss'])\n",
    "plt.plot(lstm_autoencoder_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    " # Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "\n",
    "# Set figure width to 18 and height to 4\n",
    "fig_size[0] = 18\n",
    "fig_size[1] = 4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
