{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard,CSVLogger\n",
    "from keras.models import load_model\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(7)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(11)\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123 #used to help randomly select the data points\n",
    "DATA_SPLIT_PCT = 0.2\n",
    "\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "LABELS = [\"Normal\",\"Break\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  dataset\n",
    "df = pd.read_csv('/home/francovm/Projects/SSE/data/processed/input_data_shifted_40.csv', sep='\\t', encoding='utf-8', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = df.loc[:, df.columns != 'Events'].values  # converts the df to a numpy array\n",
    "input_y = df['Events'].values\n",
    "\n",
    "n_features = input_X.shape[1]  # number of features\n",
    "\n",
    "print(input_X.shape,input_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporalize(X, y, lookback):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(input_X)-lookback-1):\n",
    "        t = []\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            t.append(input_X[[(i+j+1)], :])\n",
    "        X.append(t)\n",
    "        y.append(input_y[i+lookback+1])\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporalize the data\n",
    "lookback = 20\n",
    "X, y = temporalize(X = input_X, y = input_y, lookback = lookback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into Test, valid and train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=DATA_SPLIT_PCT, random_state=SEED)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=DATA_SPLIT_PCT, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the arrays\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], lookback, n_features)\n",
    "# X_train_y0 = X_train_y0.reshape(X_train_y0.shape[0], lookback, n_features)\n",
    "# X_train_y1 = X_train_y1.reshape(X_train_y1.shape[0], lookback, n_features)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], lookback, n_features)\n",
    "# X_valid_y0 = X_valid_y0.reshape(X_valid_y0.shape[0], lookback, n_features)\n",
    "# X_valid_y1 = X_valid_y1.reshape(X_valid_y1.shape[0], lookback, n_features)b\n",
    "X_test = X_test.reshape(X_test.shape[0], lookback, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Categorical Data\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n",
    "# y_valid = to_categorical(y_valid)\n",
    "\n",
    "# print(y_valid.shape, y_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 1\n",
    "print(n_timesteps,n_features,n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    '''\n",
    "    Flatten a 3D array.\n",
    "    \n",
    "    Input\n",
    "    X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
    "    \n",
    "    Output\n",
    "    flattened_X  A 2D array, sample x features.\n",
    "    '''\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "\n",
    "def scale(X, scaler):\n",
    "    '''\n",
    "    Scale 3D array.\n",
    "\n",
    "    Inputs\n",
    "    X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
    "    scaler       A scaler object, e.g., sklearn.preprocessing.StandardScaler, sklearn.preprocessing.normalize\n",
    "    \n",
    "    Output\n",
    "    X            Scaled 3D array.\n",
    "    '''\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i, :, :] = scaler.transform(X[i, :, :])\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stardarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a scaler using the training data.\n",
    "# scaler = StandardScaler().fit(flatten(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the input\n",
    "\n",
    "# X_train_scaled = scale(X_train, scaler)\n",
    "# X_valid_scaled = scale(X_valid, scaler)\n",
    "# X_test_scaled = scale(X_test, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = flatten(X_train_scaled)\n",
    "# print('colwise mean', np.mean(a,axis=0).round(6))\n",
    "# print('colwise variance', np.var(a, axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test acurracy\n",
    "\n",
    "Not use for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "# def evaluate_model(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "#     verbose, epochs, batch_size = 1, 15, 64\n",
    "#     n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "# #     model.add(Dense(n_outputs, activation='softmax'))\n",
    "#     model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#     # fit network\n",
    "#     model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size ,validation_data=(X_valid, y_valid),verbose=verbose)\n",
    "#     # evaluate model\n",
    "#     _, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "# def summarize_results(scores):\n",
    "#     print(scores)\n",
    "#     m, s = np.mean(scores), np.std(scores)\n",
    "#     print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment\n",
    "# def run_experiment(repeats=2):\n",
    "#     # repeat experiment\n",
    "#     scores = list()\n",
    "#     for r in range(repeats):\n",
    "#         score = evaluate_model(X_train, y_train, X_test, y_test)\n",
    "#         score = score * 100.0\n",
    "#         print('>#%d: %.3f' % (r+1, score))\n",
    "#         scores.append(score)\n",
    "#     # summarize results\n",
    "#     summarize_results(scores)\n",
    " \n",
    "    # run the experiment\n",
    "# run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "Training models (Use it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "verbose, epochs, batch_size = 1, 100, 64\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "#     model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "cp = ModelCheckpoint(filepath=\"/home/francovm/Projects/SSE/models/Binary_clasifier/Binary_clasifier_LSTM_SSE_40days-shifted.h5\",\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)\n",
    "\n",
    "tb = TensorBoard(log_dir='/home/francovm/Projects/SSE/models/Binary_clasifier/Tensorboard/Binary_clasifier_LSTM_SSE_40days-shifted',\n",
    "                histogram_freq=0,\n",
    "                write_graph=True,\n",
    "                write_images=True)\n",
    "\n",
    "csv_logger = CSVLogger('/home/francovm/Projects/SSE/data/Visualization/Data/Binary_clasifier_LSTM_SSE_40days-shifted.csv', append=True, separator=';')\n",
    "\n",
    "#set early stopping monitor so the model stops training when it won't improve anymore\n",
    "early_stopping_monitor = EarlyStopping(patience=5)\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_valid, y_valid), verbose=verbose,\n",
    "                    callbacks=[early_stopping_monitor,csv_logger,tb,cp])\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "#  Set figure width to 18 and height to 4\n",
    "fig_size[0] = 18\n",
    "fig_size[1] = 4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "plt.savefig('/home/francovm/Projects/SSE/data/Visualization/LSTM-40days-acc-shifted.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    " # Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "\n",
    "# Set figure width to 18 and height to 4\n",
    "fig_size[0] = 18\n",
    "fig_size[1] = 4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "plt.savefig('/home/francovm/Projects/SSE/data/Visualization/LSTM-40days-loss-shifted_.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "# model.save(\"/home/francovm/Projects/SSE/models/Binary_clasifier/Binary_clasifier_LSTM_SSE_40days-shifted.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('/home/francovm/Projects/SSE/models/Binary_clasifier/Binary_clasifier_LSTM_SSE_40days-shifted.h5')\n",
    "# summarize model.\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(X_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, yhat_probs)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "auc = roc_auc_score(y_test, yhat_probs)\n",
    "print('AUC: %.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "# probs = model.predict_proba(X_test)\n",
    "# preds = probs[:,1.]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, yhat_probs)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
