{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard,CSVLogger\n",
    "from keras.models import load_model\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(7)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123 #used to help randomly select the data points\n",
    "DATA_SPLIT_PCT = 0.2\n",
    "\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "LABELS = [\"Normal\",\"Break\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  dataset\n",
    "df = pd.read_csv('/home/francovm/Projects/SSE/data/processed/input_data.csv', sep='\\t', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = df.loc[:, df.columns != 'Events'].values  # converts the df to a numpy array\n",
    "input_y = df['Events'].values\n",
    "\n",
    "n_features = input_X.shape[1]  # number of features\n",
    "\n",
    "print(input_X.shape,input_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporalize(X, y, lookback):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(input_X)-lookback-1):\n",
    "        t = []\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            t.append(input_X[[(i+j+1)], :])\n",
    "        X.append(t)\n",
    "        y.append(input_y[i+lookback+1])\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporalize the data\n",
    "lookback = 40\n",
    "X, y = temporalize(X = input_X, y = input_y, lookback = lookback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into Test, valid and train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=DATA_SPLIT_PCT, random_state=SEED)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=DATA_SPLIT_PCT, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_y0 = X_train[y_train==0.]\n",
    "X_train_y1 = X_train[y_train==1.]\n",
    "\n",
    "X_valid_y0 = X_valid[y_valid==0.]\n",
    "X_valid_y1 = X_valid[y_valid==1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the arrays\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], lookback, n_features)\n",
    "X_train_y0 = X_train_y0.reshape(X_train_y0.shape[0], lookback, n_features)\n",
    "X_train_y1 = X_train_y1.reshape(X_train_y1.shape[0], lookback, n_features)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], lookback, n_features)\n",
    "X_valid_y0 = X_valid_y0.reshape(X_valid_y0.shape[0], lookback, n_features)\n",
    "X_valid_y1 = X_valid_y1.reshape(X_valid_y1.shape[0], lookback, n_features)\n",
    "X_test = X_test.reshape(X_test.shape[0], lookback, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Categorical Data\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n",
    "# y_valid = to_categorical(y_valid)\n",
    "\n",
    "# print(y_valid.shape, y_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 1\n",
    "print(n_timesteps,n_features,n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    '''\n",
    "    Flatten a 3D array.\n",
    "    \n",
    "    Input\n",
    "    X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
    "    \n",
    "    Output\n",
    "    flattened_X  A 2D array, sample x features.\n",
    "    '''\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "\n",
    "def scale(X, scaler):\n",
    "    '''\n",
    "    Scale 3D array.\n",
    "\n",
    "    Inputs\n",
    "    X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
    "    scaler       A scaler object, e.g., sklearn.preprocessing.StandardScaler, sklearn.preprocessing.normalize\n",
    "    \n",
    "    Output\n",
    "    X            Scaled 3D array.\n",
    "    '''\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i, :, :] = scaler.transform(X[i, :, :])\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stardarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a scaler using the training data.\n",
    "scaler = StandardScaler().fit(flatten(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the input\n",
    "X_train_y0_scaled = scale(X_train_y0, scaler)\n",
    "X_valid_y0_scaled = scale(X_valid_y0, scaler)\n",
    "X_test_scaled = scale(X_test, scaler)\n",
    "X_train_scaled = scale(X_train, scaler)\n",
    "X_valid_scaled = scale(X_valid, scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = flatten(X_train_scaled)\n",
    "print('colwise mean', np.mean(a,axis=0).round(6))\n",
    "print('colwise variance', np.var(a, axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "Training models (Use it)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "verbose, epochs, batch_size = 1, 400, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_autoencoder = Sequential()\n",
    "# Encoder\n",
    "lstm_autoencoder.add(LSTM(256, activation='relu', input_shape=(n_timesteps, n_features), return_sequences=True))\n",
    "lstm_autoencoder.add(LSTM(128, activation='relu', return_sequences=False))\n",
    "lstm_autoencoder.add(RepeatVector(n_timesteps))\n",
    "# Decoder\n",
    "lstm_autoencoder.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "lstm_autoencoder.add(LSTM(256, activation='relu', return_sequences=True))\n",
    "lstm_autoencoder.add(TimeDistributed(Dense(n_features)))\n",
    "\n",
    "lstm_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = optimizers.Adam(lr)\n",
    "# lstm_autoencoder.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# cp = ModelCheckpoint(filepath=\"/home/francovm/Projects/SSE/models/Autoencoder/big256-Autoencoder-classifier_LSTM_SSE_20window-stand.h5\",\n",
    "#                                save_best_only=True,\n",
    "#                                verbose=0)\n",
    "\n",
    "# tb = TensorBoard(log_dir='/home/francovm/Projects/SSE/models/Autoencoder/Tensorboard/big256-Autoencoder-classifier_LSTM_SSE_20window-stand',\n",
    "#                 histogram_freq=0,\n",
    "#                 write_graph=True,\n",
    "#                 write_images=True)\n",
    "\n",
    "# #set early stopping monitor so the model stops training when it won't improve anymore\n",
    "# early_stopping_monitor = EarlyStopping(patience=5)\n",
    "\n",
    "# csv_logger = CSVLogger('/home/francovm/Projects/SSE/data/Visualization/Data/big256-Autoencoder-classifier_LSTM_SSE_20window-stand.csv', append=True, separator=';')\n",
    "\n",
    "# lstm_autoencoder_history = lstm_autoencoder.fit(X_train_y0_scaled, X_train_y0_scaled, \n",
    "#                                                 epochs=epochs, \n",
    "#                                                 batch_size=batch_size, \n",
    "#                                                 callbacks=[early_stopping_monitor,csv_logger,cp,tb],\n",
    "#                                                 validation_data=(X_valid_y0_scaled, X_valid_y0_scaled),\n",
    "#                                                 verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(lstm_autoencoder_history.history.keys())\n",
    "\n",
    "# # summarize history for accuracy\n",
    "plt.plot(lstm_autoencoder_history.history['accuracy'])\n",
    "plt.plot(lstm_autoencoder_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "#  Set figure width to 18 and height to 4\n",
    "fig_size[0] = 18\n",
    "fig_size[1] = 4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "plt.savefig('/home/francovm/Projects/SSE/data/Visualization/big256-LSTM-20days-acc-Autoencoder-classifier-stand.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(lstm_autoencoder_history.history['loss'])\n",
    "plt.plot(lstm_autoencoder_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    " # Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "\n",
    "# Set figure width to 18 and height to 4\n",
    "fig_size[0] = 18\n",
    "fig_size[1] = 4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "plt.savefig('/home/francovm/Projects/SSE/data/Visualization/big256-LSTM-20days-loss-Autoencoder-classifier-stand.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "\n",
    "# lstm_autoencoder.save(\"/home/francovm/Projects/SSE/models/Autoencoder/LSTM_SSE_20days-Classification.h5\")\n",
    "\n",
    "# df = pd.DataFrame(lstm_autoencoder_history.history)\n",
    "\n",
    "# df.to_csv( '/home/francovm/Projects/SSE/data/Visualization/Data/LSTM_SSE_20days_Autoencoder_Classification.csv',header=True)\n",
    "\n",
    "\n",
    "\n",
    "# load model\n",
    "model = load_model('/home/francovm/Projects/SSE/models/Autoencoder/Autoencoder-classifier_LSTM_SSE_40window-stand.h5')\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check\n",
    "\n",
    "Doing a sanity check by validating the reconstruction error on the train data. Here we will reconstruct the entire train data with both 0 and 1 labels.\n",
    "\n",
    "**Expectation**: the reconstruction error of 0 labeled data should be smaller than 1.\n",
    "\n",
    "**Caution**: do not use this result for model evaluation. It may result into overfitting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_predictions = model.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mse = np.mean(np.power(flatten(X_train) - flatten(train_x_predictions), 2), axis=1)\n",
    "\n",
    "# error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "#                         'True_class': y_train.tolist()})\n",
    "\n",
    "# # error_df = error_df.reset_index()\n",
    "# groups = error_df.groupby('True_class')\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # print(np.reshape(group.index,140681,1), group.Reconstruction_error)\n",
    "# for name, group in groups:\n",
    "#     ax.plot(group.Reconstruction_error, marker='o', ms=3.5, linestyle='',label= \"Break\" if name == 1. else \"Normal\")\n",
    "\n",
    "# ax.legend()\n",
    "# plt.ylim(0,1)\n",
    "# plt.title(\"Reconstruction error for different classes\")\n",
    "# plt.ylabel(\"Reconstruction error\")\n",
    "# plt.xlabel(\"Data point index\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions using the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_predictions = model.predict(X_valid_scaled)\n",
    "\n",
    "# valid_x_predictions = lstm_autoencoder.predict(X_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_x_predictions = lstm_autoencoder.predict(X_valid)\n",
    "mse = np.mean(np.power(flatten(X_valid_scaled) - flatten(valid_x_predictions), 2), axis=1)\n",
    "\n",
    "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                        'True_class': y_valid.tolist()})\n",
    "\n",
    "precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)\n",
    "plt.plot(threshold_rt, precision_rt[1:], label=\"Precision\",linewidth=5)\n",
    "plt.plot(threshold_rt, recall_rt[1:], label=\"Recall\",linewidth=5)\n",
    "plt.title('Precision and recall for different threshold values')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,9),dpi=80)\n",
    "# plt.title('loss Distribution', fontsize=16)\n",
    "# sns.distplot(error_df['Reconstruction_error'],bins=100,kde=True,color='blue');\n",
    "# plt.xlim([0.0,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold=0\n",
    "# f1=0\n",
    "# recall=0\n",
    "# accuracy=0\n",
    "# while (recall < 0.5 or accuracy < 0.6):\n",
    "#     print ('**************************')\n",
    "#     print (threshold)\n",
    "# #     threshold += .0005\n",
    "#     threshold += 1000\n",
    "#     y_pred = [1 if e > threshold else 0 for e in error_df.Reconstruction_error.values]\n",
    "#     conf_matrix = confusion_matrix(error_df.True_class, y_pred)\n",
    "#     tn, fp, fn, tp = conf_matrix.ravel()\n",
    "#     precision = 1. * tp / (tp + fp)\n",
    "#     recall = 1. * tp / (tp + fn)\n",
    "#     f1 = (2 * recall * precision) / (recall + precision)\n",
    "# # print ('**************************')\n",
    "# # print (threshold)\n",
    "#     print ('TP:' + str(tp))\n",
    "#     print ('FP:' + str(fp))\n",
    "#     print ('TN:' + str(tn))\n",
    "#     print ('FN:' + str(fn))\n",
    "#     accuracy = 1. * (tp + tn) / (tp + tn + fp + fn)\n",
    "#     print ('Accuracy:' + str(accuracy))\n",
    "#     print ('Precision:' + str(precision))\n",
    "#     print ('Recall:' + str(recall))\n",
    "#     print ('F1:' + str(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_predictions = model.predict(X_test_scaled)\n",
    "# test_x_predictions = lstm_autoencoder.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x_predictions = lstm_autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(flatten(X_test_scaled) - flatten(test_x_predictions), 2), axis=1)\n",
    "\n",
    "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                        'True_class': y_test.tolist()})\n",
    "\n",
    "threshold_fixed = 0.002 # Definir!\n",
    "groups = error_df.groupby('True_class')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot( group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "            label= \"Break\" if name == 1 else \"Normal\")\n",
    "ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "\n",
    "ax.legend()\n",
    "plt.ylim(0,0.1)\n",
    "plt.title(\"Reconstruction error for different classes\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = [1. if e > threshold_fixed else 0. for e in error_df.Reconstruction_error.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(error_df.True_class, pred_y)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "ax.set_ylim(2, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.True_class, error_df.Reconstruction_error)\n",
    "roc_auc = auc(false_pos_rate, true_pos_rate,)\n",
    "\n",
    "plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\n",
    "plt.plot([0,1],[0,1], linewidth=5)\n",
    "\n",
    "plt.xlim([-0.01, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver operating characteristic curve (ROC)')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
